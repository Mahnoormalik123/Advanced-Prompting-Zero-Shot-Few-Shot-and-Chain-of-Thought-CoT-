{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d581e1",
   "metadata": {},
   "source": [
    "\n",
    "# Week 3 Assignment  \n",
    "**Advanced Prompting: Zero-Shot, Few-Shot, and Chain-of-Thought (CoT)**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa77795",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset 1: Logic Puzzles  \n",
    "\n",
    "### Puzzle 1: Alice is older than Bob. Bob is older than Charlie. Who is the youngest?\n",
    "\n",
    "| Approach     | Prompt Used | Model Output |\n",
    "|--------------|-------------|--------------|\n",
    "| Zero-Shot    | Alice is older than Bob. Bob is older than Charlie. Who is the youngest? | Charlie. Since Alice > Bob and Bob > Charlie, transitivity gives Alice > Bob > Charlie — so Charlie is the youngest. |\n",
    "| Few-Shot     | Example 1:<br>Input: \"Tom is taller than Jerry. Jerry is taller than Spike. Who is the tallest?\"<br>Output: Tom<br><br>Example 2:<br>Input: \"Sara is smarter than Ali. Ali is smarter than Zoya. Who is the smartest?\"<br>Output: Sara<br><br>Now answer this:<br>Input: \"Alice is older than Bob. Bob is older than Charlie. Who is the youngest?\"<br>Output: | Charlie |\n",
    "| CoT          | Alice is older than Bob. Bob is older than Charlie. Who is the youngest?<br>Think step by step before giving the answer. | Charlie <br>**Why (brief):** From “Alice is older than Bob” and “Bob is older than Charlie” we get the order Alice > Bob > Charlie, so Charlie is the youngest.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12eb51e",
   "metadata": {},
   "source": [
    "\n",
    "### Puzzle 2: There are three boxes: one with apples, one with oranges, one mixed. All are mislabeled. You can take one fruit from one box. How do you correctly label all?\n",
    "\n",
    "| Approach  | Prompt Used | Model Output |\n",
    "|-----------|-------------|--------------|\n",
    "| **Zero-Shot** | *There are three boxes: one with apples, one with oranges, one mixed. All are mislabeled. You can take one fruit from one box. How do you correctly label all?* | Pick one fruit from the box labeled \"Mixed\".<br><br>**If the fruit is an apple:**<br>- The box you drew from is the **Apples** box.<br>- The box labeled \"Apples\" cannot be Apples (labels are wrong) and cannot be Mixed (we already found the Apples box), so it must be **Oranges**.<br>- The remaining box (labeled \"Oranges\") must therefore be **Mixed**.<br><br>**If the fruit is an orange:**<br>- The box you drew from is the **Oranges** box.<br>- The box labeled \"Oranges\" must be **Apples**.<br>- The remaining box (labeled \"Apples\") must be **Mixed**.<br><br> One pick from the \"Mixed\" box lets you deduce all correct labels. |\n",
    "| **Few-Shot** | *Example 1:*<br>Input: \"There are two doors: one always lies, one always tells the truth. You can ask one question. How do you find the safe door?\"<br>Output: Ask: \"If I ask the other guard which door leads to safety, what would he say?\" Then choose the opposite.<br><br>*Example 2:*<br>Input: \"You have a 3-liter jug and a 5-liter jug. How to measure 4 liters exactly?\"<br>Output: Fill 5L jug → pour into 3L jug (2L left) → empty 3L jug → pour 2L into 3L → fill 5L jug again → pour into 3L until full → 4L left.<br><br>*Now answer this:*<br>Input: \"There are three boxes: one with apples, one with oranges, one mixed. All are mislabeled. You can take one fruit from one box. How do you correctly label all?\" | Take one fruit from the box labeled \"Mixed\".<br>If apple → that box is **Apples** → the box labeled \"Apples\" must be **Oranges** → the remaining (labeled \"Oranges\") is **Mixed**.<br>If orange → that box is **Oranges** → the box labeled \"Oranges\" must be **Apples** → the remaining (labeled \"Apples\") is **Mixed**.<br><br> This single pick allows correct labeling of all three boxes. |\n",
    "| **CoT** | *There are three boxes: one with apples, one with oranges, one mixed. All are mislabeled. You can take one fruit from one box. Think step by step and explain.* | 1. Pick one fruit from the box labeled \"Mixed\".<br>2. Since all labels are wrong, this box must contain only Apples or only Oranges.<br>3. **Case 1:** If you pick an apple → that box is Apples → the box labeled \"Apples\" must be Oranges → the box labeled \"Oranges\" must be Mixed.<br>4. **Case 2:** If you pick an orange → that box is Oranges → the box labeled \"Oranges\" must be Apples → the box labeled \"Apples\" must be Mixed.<br><br> One pick from the \"Mixed\" box is enough to relabel all three boxes correctly. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1de573",
   "metadata": {},
   "source": [
    "### Puzzle 3: If all Bloops are Razzies and all Razzies are Lazzies, are all Bloops definitely Lazzies?\n",
    "\n",
    "| Approach  | Prompt Used | Model Output |\n",
    "|-----------|-------------|--------------|\n",
    "| **Zero-Shot** | *If all Bloops are Razzies and all Razzies are Lazzies, are all Bloops definitely Lazzies?* | Yes.<br><br> All Bloops ⊆ Razzies, and all Razzies ⊆ Lazzies → by transitivity, all Bloops ⊆ Lazzies. |\n",
    "| **Few-Shot** | *Example 1:*<br>Input: \"All cats are animals. All animals are living beings. Are all cats living beings?\"<br>Output: Yes.<br><br>*Example 2:*<br>Input: \"All roses are flowers. All flowers are plants. Are all roses plants?\"<br>Output: Yes.<br><br>*Now answer this:*<br>Input: \"If all Bloops are Razzies and all Razzies are Lazzies, are all Bloops definitely Lazzies?\" | Yes.<br><br> Because the relationship is transitive: Bloops → Razzies → Lazzies. |\n",
    "| **CoT** | *If all Bloops are Razzies and all Razzies are Lazzies, are all Bloops definitely Lazzies? Think step by step and explain.* | Step by step:<br>1. All Bloops are Razzies (Bloops ⊆ Razzies).<br>2. All Razzies are Lazzies (Razzies ⊆ Lazzies).<br>3. If Bloops are contained within Razzies, and Razzies are contained within Lazzies, then Bloops are also contained within Lazzies.<br> Therefore, the answer is **Yes, all Bloops are definitely Lazzies**. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740df28",
   "metadata": {},
   "source": [
    "### Math Problem 1: A train travels 60 km in 1 hour. How far will it go in 4 hours?\n",
    "\n",
    "| Approach  | Prompt Used | Model Output |\n",
    "|-----------|-------------|--------------|\n",
    "| **Zero-Shot** | *A train travels 60 km in 1 hour. How far will it go in 4 hours?* | 240 km. |\n",
    "| **Few-Shot** | *Example 1:*<br>Input: \"A car travels 50 km in 1 hour. How far will it go in 3 hours?\"<br>Output: 150 km.<br><br>*Example 2:*<br>Input: \"A bike moves 20 km in 1 hour. How far in 5 hours?\"<br>Output: 100 km.<br><br>*Now answer this:*<br>Input: \"A train travels 60 km in 1 hour. How far will it go in 4 hours?\" | 240 km.<br><br> Because 60 × 4 = 240. |\n",
    "| **CoT** | *A train travels 60 km in 1 hour. How far will it go in 4 hours? Think step by step.* | Step by step:<br>1. Train speed = 60 km/h.<br>2. Time = 4 hours.<br>3. Distance = speed × time = 60 × 4.<br> Therefore, the train will travel **240 km**. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9ed80",
   "metadata": {},
   "source": [
    "### Math Problem 2: What is 23 × 17?\n",
    "\n",
    "| Approach  | Prompt Used | Model Output |\n",
    "|-----------|-------------|--------------|\n",
    "| **Zero-Shot** | *What is 23 × 17?* | 391 |\n",
    "| **Few-Shot** | *Example 1:*<br>Input: \"What is 12 × 5?\"<br>Output: 60.<br><br>*Example 2:*<br>Input: \"What is 30 × 4?\"<br>Output: 120.<br><br>*Now answer this:*<br>Input: \"What is 23 × 17?\" | 391 |\n",
    "| **CoT** | *What is 23 × 17? Think step by step.* | Step by step:<br>1. Break 17 into (10 + 7).<br>2. 23 × 10 = 230.<br>3. 23 × 7 = 161.<br>4. Add them: 230 + 161 = 391.<br> Final Answer = **391** |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5bc2f2",
   "metadata": {},
   "source": [
    "### Math Problem 3: Pizza Slices (Fraction Problem)\n",
    "\n",
    "| Approach  | Prompt Used | Model Output |\n",
    "|-----------|-------------|--------------|\n",
    "| **Zero-Shot** | *A pizza is cut into 8 equal slices. You eat 3 slices. What fraction remains?* | 5/8 |\n",
    "| **Few-Shot** | *Example 1:*<br>Input: \"A cake has 10 pieces. You eat 2. What fraction remains?\"<br>Output: 8/10 or 4/5.<br><br>*Example 2:*<br>Input: \"A chocolate bar is divided into 12 parts. You eat 6. What fraction remains?\"<br>Output: 6/12 or 1/2.<br><br>*Now answer this:*<br>Input: \"A pizza is cut into 8 slices. You eat 3. What fraction remains?\" | 5/8 |\n",
    "| **CoT** | *A pizza is cut into 8 equal slices. You eat 3. What fraction remains? Think step by step.* | Step by step:<br>1. Total slices = 8.<br>2. Slices eaten = 3.<br>3. Remaining = 8 - 3 = 5.<br>4. Fraction = 5/8.<br> Final Answer = **5/8** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9379ad7",
   "metadata": {},
   "source": [
    "## Dataset 3: Reasoning Tasks  \n",
    "\n",
    "### Task 1: If John is taller than Mary, and Mary is taller than Sam, who is the tallest?\n",
    "\n",
    "| Approach     | Prompt Used | Model Output |\n",
    "|--------------|-------------|--------------|\n",
    "| Zero-Shot    | If John is taller than Mary, and Mary is taller than Sam, who is the tallest? | Step-by-step: <br>John is taller than Mary.<br>Mary is taller than Sam.<br>Therefore John is taller than both Mary and Sam.<br>Answer: John is the tallest. |\n",
    "| Few-Shot     | Example 1:<br>Input: \"Ali is stronger than Bilal. Bilal is stronger than Hamza. Who is the strongest?\"<br>Output: Ali<br><br>Example 2:<br>Input: \"Sara is faster than Aisha. Aisha is faster than Hina. Who is the fastest?\"<br>Output: Sara<br><br>Now answer this:<br>Input: \"If John is taller than Mary, and Mary is taller than Sam, who is the tallest?\" | John |\n",
    "| CoT          | If John is taller than Mary, and Mary is taller than Sam, who is the tallest? Think step by step before answering. | John is taller than Mary.<br>Mary is taller than Sam.<br>So John is taller than both Mary and Sam.<br>Answer: John |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e613d3",
   "metadata": {},
   "source": [
    "### Task 2: A farmer has 5 cows, each produces 8 liters of milk per day. How much milk in total per day?\n",
    "\n",
    "| Approach     | Prompt Used | Model Output |\n",
    "|--------------|-------------|--------------|\n",
    "| Zero-Shot    | A farmer has 5 cows, each produces 8 liters of milk per day. How much milk in total per day? | Step by step:<br>Number of cows = 5<br>Milk per cow per day = 8 liters<br>Total milk = 5 × 8 = 40 liters<br> The farmer gets 40 liters of milk per day. |\n",
    "| Few-Shot     | Example 1:<br>Input: \"A farmer has 4 hens, each lays 2 eggs per day. How many eggs in total per day?\"<br>Output: 8<br><br>Example 2:<br>Input: \"A factory produces 3 cars per day. In 7 days, how many cars?\"<br>Output: 21<br><br>Now answer this:<br>Input: \"A farmer has 5 cows, each produces 8 liters of milk per day. How much milk in total per day?\" | 40 |\n",
    "| CoT          | A farmer has 5 cows, each produces 8 liters of milk per day. How much milk in total per day? Think step by step and then answer. | Number of cows = 5<br>Milk per cow per day = 8 liters<br>Total milk = 5 × 8 = 40<br>Answer: 40 liters per day |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e6a7cb",
   "metadata": {},
   "source": [
    "\n",
    "### Task 3: If it is raining, the ground will be wet. The ground is wet. Does it mean it rained?\n",
    "\n",
    "| Approach     | Prompt Used | Model Output |\n",
    "|--------------|-------------|--------------|\n",
    "| Zero-Shot    | If it is raining, the ground will be wet. The ground is wet. Does it mean it rained? | Not necessarily.<br><br>Statement: If it is raining, then the ground will be wet.<br>This only guarantees that rain → wet ground.<br>But the reverse (wet ground → rain) is not always true. The ground might be wet for other reasons (sprinklers, someone washing it, etc.).<br>So, the ground being wet does not prove it rained. |\n",
    "| Few-Shot     | Example 1:<br>Input: \"If it is snowing, the road will be slippery. The road is slippery. Does it mean it snowed?\"<br>Output: Not necessarily (other causes possible).<br><br>Example 2:<br>Input: \"If the alarm rings, class will end. Class has ended. Does it mean alarm rang?\"<br>Output: Not necessarily (teacher may have dismissed early).<br><br>Now answer this:<br>Input: \"If it is raining, the ground will be wet. The ground is wet. Does it mean it rained?\" | Not necessarily (the ground could be wet for other reasons). |\n",
    "| CoT          | If it is raining, the ground will be wet. The ground is wet. Does it mean it rained? Think step by step before answering. | Premise: If it is raining → the ground will be wet.<br>Observation: The ground is wet.<br>This does not guarantee it rained (affirming the consequent). The ground could be wet for other reasons — sprinklers, someone washing, dew, a leak, etc.<br>Answer: Not necessarily. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376b232",
   "metadata": {},
   "source": [
    "# Reasoning Evaluation Rubric Scores\n",
    "\n",
    "| Dataset | Task | Approach | Correctness (0-3) | Reasoning Clarity (0-3) | Completeness (0-3) | Conciseness (0-3) | Total (0-12) | Interpretation |\n",
    "|---------|------|----------|------------------|-------------------------|------------------|-----------------|---------------|----------------|\n",
    "| Logic Puzzles | Puzzle 1: Alice/Bob/Charlie | Zero-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Logic Puzzles | Puzzle 1: Alice/Bob/Charlie | Few-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Logic Puzzles | Puzzle 1: Alice/Bob/Charlie | CoT | 3 | 3 | 3 | 3 | 12 | Excellent reasoning |\n",
    "| Logic Puzzles | Puzzle 2: 3 boxes | Zero-Shot | 3 | 2 | 2 | 2 | 9 | Good but improvable |\n",
    "| Logic Puzzles | Puzzle 2: 3 boxes | Few-Shot | 3 | 2 | 3 | 3 | 11 | Excellent reasoning |\n",
    "| Logic Puzzles | Puzzle 2: 3 boxes | CoT | 3 | 3 | 3 | 3 | 12 | Excellent reasoning |\n",
    "| Logic Puzzles | Puzzle 3: Bloops/Razzies/Lazzies | Zero-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Logic Puzzles | Puzzle 3: Bloops/Razzies/Lazzies | Few-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Logic Puzzles | Puzzle 3: Bloops/Razzies/Lazzies | CoT | 3 | 3 | 3 | 3 | 12 | Excellent reasoning |\n",
    "| Math Problems | Problem 1: Train distance | Zero-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Math Problems | Problem 1: Train distance | Few-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Math Problems | Problem 1: Train distance | CoT | 3 | 3 | 3 | 3 | 12 | Excellent reasoning |\n",
    "| Math Problems | Problem 2: 23×17 | Zero-Shot | 3 | 2 | 3 | 2 | 10 | Excellent reasoning |\n",
    "| Math Problems | Problem 2: 23×17 | Few-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Math Problems | Problem 2: 23×17 | CoT | 3 | 3 | 3 | 3 | 12 | Excellent reasoning |\n",
    "| Math Problems | Problem 3: Pizza fraction | Zero-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Math Problems | Problem 3: Pizza fraction | Few-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Math Problems | Problem 3: Pizza fraction | CoT | 3 | 3 | 3 | 3 | 12 | Excellent reasoning |\n",
    "| Reasoning Tasks | Task 1: John/Mary/Sam | Zero-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Reasoning Tasks | Task 1: John/Mary/Sam | Few-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Reasoning Tasks | Task 1: John/Mary/Sam | CoT | 3 | 3 | 3 | 3 | 12 | Excellent reasoning |\n",
    "| Reasoning Tasks | Task 2: Milk from cows | Zero-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Reasoning Tasks | Task 2: Milk from cows | Few-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Reasoning Tasks | Task 2: Milk from cows | CoT | 3 | 3 | 3 | 3 | 12 | Excellent reasoning |\n",
    "| Reasoning Tasks | Task 3: Wet ground | Zero-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Reasoning Tasks | Task 3: Wet ground | Few-Shot | 3 | 2 | 2 | 3 | 10 | Excellent reasoning |\n",
    "| Reasoning Tasks | Task 3: Wet ground | CoT | 3 | 3 | 3 | 3 | 12 | Excellent reasoning |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331ba90",
   "metadata": {},
   "source": [
    "## Comparative Insights\n",
    "\n",
    "After completing all tasks using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting, several clear patterns emerged regarding performance, reasoning quality, and clarity:\n",
    "\n",
    "- **Zero-Shot Prompting:**  \n",
    "  This approach is the fastest and requires minimal setup since no examples are provided. For simple or straightforward tasks—like basic arithmetic or single-step logic puzzles—Zero-Shot usually produced correct answers. However, for more complex or multi-step problems, the model often skipped intermediate reasoning steps, which can make the output harder to follow and less transparent. While Zero-Shot can be effective for quick answers, it does not explicitly show the reasoning process, which is crucial in understanding *how* a conclusion was reached.\n",
    "\n",
    "- **Few-Shot Prompting:**  \n",
    "  Including two or more examples allows the model to identify patterns and understand the expected structure of reasoning. Few-Shot prompts consistently improved accuracy for tasks that follow a recognizable pattern, such as transitive logic in puzzles or repeated arithmetic calculations. It also encouraged the model to mimic the reasoning style shown in the examples. That said, Few-Shot can still fall short when novel or more complicated reasoning is required, as it relies heavily on pattern matching rather than independent step-by-step logic.\n",
    "\n",
    "- **Chain-of-Thought (CoT) Prompting:**  \n",
    "  CoT prompting explicitly instructs the model to “think step by step,” which significantly enhances reasoning transparency. Across all datasets, CoT consistently produced outputs with the clearest, most detailed reasoning. It helped prevent common errors in multi-step logic puzzles, complex arithmetic, or problems requiring inference. Even when the answer could be easily guessed, CoT made the thought process explicit, demonstrating how conclusions are reached from the premises. This approach was especially valuable for logic puzzles and reasoning tasks where explaining *why* an answer is correct is just as important as the answer itself.\n",
    "\n",
    "**Overall Pattern:**  \n",
    "CoT prompting generally outperformed both Zero-Shot and Few-Shot in terms of reasoning clarity, completeness, and reliability. Few-Shot performed well when the task pattern matched the examples, but it was less consistent on tasks requiring novel reasoning. Zero-Shot was adequate for simple questions but often lacked explanation and transparency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83ae44",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Reflecting on the outcomes of this lab, several insights stand out:\n",
    "\n",
    "1. **Accuracy vs Transparency:**  \n",
    "   While Zero-Shot can produce correct answers quickly, the lack of reasoning makes it harder to verify or trust the solution in complex tasks. CoT provides both accuracy and transparency, making it the preferred method for multi-step reasoning problems.\n",
    "\n",
    "2. **Influence of Examples:**  \n",
    "   Few-Shot prompts clearly demonstrate that examples influence model behavior, helping it recognize patterns and align responses with the expected format. However, relying too heavily on examples can make the model less flexible in handling novel scenarios that differ from the provided examples.\n",
    "\n",
    "3. **Reasoning Quality:**  \n",
    "   CoT dramatically improves reasoning quality by forcing the model to articulate intermediate steps. This not only reduces errors but also allows a human reviewer to follow the logic, which is critical for tasks that require validation or explanation.\n",
    "\n",
    "4. **Practical Implications:**  \n",
    "   In real-world applications, choosing the right prompting strategy depends on task complexity. For simple fact-based questions, Zero-Shot is sufficient. For structured pattern-based tasks, Few-Shot is helpful. For multi-step reasoning, problem-solving, or when transparency is essential, CoT is clearly superior.\n",
    "\n",
    "5. **Key Takeaways:**  \n",
    "   - CoT > Few-Shot > Zero-Shot in overall reliability and reasoning clarity.  \n",
    "   - Providing examples improves response structure but cannot replace explicit step-by-step reasoning.  \n",
    "   - Prompt engineering is a critical skill; subtle changes (like “explain step by step” vs “give the answer”) can drastically affect output quality.\n",
    "\n",
    "**Conclusion:**  \n",
    "This lab illustrates that while all three prompting strategies have their merits, **Chain-of-Thought prompting stands out as the most robust method for reasoning-based tasks**, ensuring both accuracy and clarity. Few-Shot remains valuable for structured patterns, and Zero-Shot is best reserved for simple, direct questions where reasoning steps are less critical."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
